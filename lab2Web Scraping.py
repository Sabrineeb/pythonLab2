# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A_8tRCz7osoq3s40WrAnhg0QaEUQxenc

# **Python Web Scraping Tutorial**

# Make an HTTP request

Create an empty python file scraper.py containing the following code:
"""

def main():
    print('Hello world!')

    if __name__ == "__main__":  # Indented to match the print statement
        main()

"""# Create a virtual environment:"""

!python -m venv .venv

"""# Activate the virtual environment:"""

!.\.venv\Scripts\activate

"""# lunch the program:"""

!python scraper.py

"""# In the main function, add an url variable:"""

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  print(f"Scraping: {url}")

if __name__ == "__main__":
  main()

"""# Let’s retrieve the contents of this link, we have to install the requests library and import it:"""

!pip install requests

"""# Requests is an elegant and simple HTTP library for Python, built for human beings (docs)"""

import requests

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  response = requests.get(url)
  print(f"Scraping: {url}")
  print(response)

if __name__ == "__main__":
  main()

"""# Execute the program and you will get this result:"""

# Scraping: https://news.ycombinator.com/item?id=42919502
# <Response [200]>
# (This is a comment and not Python code, so no correction is needed)

"""# You can also show the response content and you will get all the data:"""

import requests

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  response = requests.get(url)
  print(f"Scraping: {url}")
  print(response)

  # Print response content within the main function
  print(response.content)

if __name__ == "__main__":
  main()

"""# **Parse the HTTP response with “Beautifulsoup”**

# With response.content we get all the data, we have to parse data and make it more clean and exploitable.
# For that we will use an other library called beautifulsoup:
"""

!pip install beautifulsoup4

"""To use Beautifulsoup, give it the response content and a parser (in this case it will be a html parser):"""

import requests
from bs4 import BeautifulSoup

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  response = requests.get(url)

  soup = BeautifulSoup(response.content, "html.parser")
  # find all elements with class="comment"
  elements = soup.find_all(class_="comment")

  # Show the number of elementd found
  print(f"Elements: {len(elements)}")

if __name__ == "__main__":
  main()

"""# **Extract individual comments**"""

import requests
from bs4 import BeautifulSoup

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  response = requests.get(url)

  soup = BeautifulSoup(response.content, "html.parser")
  # find all elements with class="ind" and indent level = 0
  elements = soup.find_all(class_="ind" , indent=0)
  # for each of this elements, find the next element
  comments = [e.find_next(class_="comment") for e in elements]

  # Show the number of comments found
  print(f"Comments: {len(comments)}")

if __name__ == "__main__":
  main()

"""# Execute the new code
# Replace:
"""

import requests
from bs4 import BeautifulSoup

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  response = requests.get(url)

  soup = BeautifulSoup(response.content, "html.parser")
  # find all elements with class="ind" and

"""# with


"""

import requests
from bs4 import BeautifulSoup

# Define the main function
def main():
    url = "https://news.ycombinator.com/item?id=42919502"
    response = requests.get(url)

    soup = BeautifulSoup(response.content, "html.parser")
    # Find all elements with class="ind" and indent level = 0
    elements = soup.find_all(class_="ind", indent=0)
    # For each of these elements, find the next element with class="comment"
    comments = [e.find_next(class_="comment") for e in elements]

    # Show the number of comments found
    print(f"Comments: {len(comments)}")

    # Iterate through the comments and print each one
    for comment in comments:
        print(comment)

# Execute the main function if the script is run directly
if __name__ == "__main__":
    main()

"""# **Clean up the response text**

# After executing the last modification, you will notice that still we have a lot of html tags that we want to get rid of that.
# We can use comment.get_text():
"""

import requests
from bs4 import BeautifulSoup

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  response = requests.get(url)

  soup = BeautifulSoup(response.content, "html.parser")
  # find all elements with class="ind" and indent level = 0
  elements = soup.find_all(class_="ind" , indent=0)
  # for each of this elements, find the next element
  comments = [e.find_next(class_="comment") for e in elements]

  # show each comment (job post)
  for comment in comments:
    comment_text = comment.get_text()
    print(comment_text)

if __name__ == "__main__":
  main()

"""**# Process the scraped content for useful data**"""

import requests
from bs4 import BeautifulSoup

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  response = requests.get(url)

  soup = BeautifulSoup(response.content, "html.parser")
  # find all elements with class="ind" and indent level = 0
  elements = soup.find_all(class_="ind" , indent=0)
  # for each of this elements, find the next element
  comments = [e.find_next(class_="comment") for e in elements]

  # Map of technologies keyword to search for
  # and the occurence initialized at 0
  keywords = {"python": 0, "javascript": 0, "typescript": 0, "go": 0, "c#": 0, "java": 0, "rust": 0 }

  # show each comment (job post)
  for comment in comments:
    # get the comment text and lower case it
    comment_text = comment.get_text().lower()

    # split comment by space which create an array of words
    words = comment_text.split(" ")

    print(words)

if __name__ == "__main__":
  main()

"""# Execute the code. You will notice that in the array some of words contains punctuations, and also some words that repeating so we only want to count them once.
# We definitely have to clean this up, we can do that by using the strip() function to strip the words. We do that for each word.
# After the split instruction, add:
"""

import requests
from bs4 import BeautifulSoup

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  response = requests.get(url)

  soup = BeautifulSoup(response.content, "html.parser")
  # find all elements with class="ind" and indent level = 0
  elements = soup.find_all(class_="ind" , indent=0)
  # for each of this elements, find the next element
  comments = [e.find_next(class_="comment") for e in elements]

  # Map of technologies keyword to search for
  # and the occurence initialized at 0
  keywords = {"python": 0, "javascript": 0, "typescript": 0, "go": 0, "c#": 0, "java": 0, "rust": 0 }

  # show each comment (job post)
  for comment in comments:
    # get the comment text and lower case it
    comment_text = comment.get_text().lower()

    # split comment by space which create an array of words
    words = comment_text.split(" ")

    # Use the string strip function to remove punctuation from each word
    # and place all the characters we

"""# Execute the code.
# Convert the list of words to a set to make the words unique and get rid of duplicate words, for that just replace the previous code with:
"""

import requests
from bs4 import BeautifulSoup

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  response = requests.get(url)

  soup = BeautifulSoup(response.content, "html.parser")
  # find all elements with class="ind" and indent level = 0

"""# Now that we have processed and cleaned up the scraped data, we can count our keywords inside the set. Replace the print(words) instruction with the following code:"""

import requests
from bs4 import BeautifulSoup

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  response = requests.get(url)

  soup = BeautifulSoup(response.content, "html.parser")
  # find all elements with class="ind" and indent level = 0
  elements = soup.find_all(class_="ind" , indent=0)
  # for each of this elements, find the next element
  comments = [e.find_next(class_="comment") for e in elements]

  # Map of technologies keyword to search for
  # and the occurence initialized at 0
  keywords = {"python": 0, "javascript": 0, "typescript": 0, "go": 0, "c#": 0, "java": 0, "rust": 0 }

  # show each comment (job post)
  for comment in comments:
    # get the comment text and lower case it
    comment_text = comment.get_text().lower()

    # split comment by space which create an array of words
    words = comment_text.split(" ")

    #search for k in keywords, this give you the dictionory key
    #if the key is in the words set, we add 1

"""# Just after the for comment in comments:  loop, print out the keywords:"""

import requests
from bs4 import BeautifulSoup

def main():
  url = "https://news.ycombinator.com/item?id=42919502"
  response = requests.get(url)

  soup = BeautifulSoup(response.content, "html.parser")
  # find all elements with class="ind" and indent level = 0
  elements = soup.find_all(class_="ind" , indent=0)
  # for each of this elements, find the next element
  comments = [e.find_next(class_="comment") for e in elements]

  # Map of technologies keyword to search for
  # and the occurence initialized at 0
  keywords = {"python": 0, "javascript": 0, "typescript": 0, "go": 0, "c#": 0, "java": 0, "rust": 0 }

  # show each comment (job post)
  for comment in comments:
    # get the comment text and lower case it
    comment_text = comment.get_text().lower()

    # split comment by space which create an array of words
    words = comment_text.split(" ")

    #search for k in keywords, this give you the dictionory key
    #if the key is in the words set, we add 1
    # Iterate through keywords and increment count if found in words
    for k in keywords:
        if k in words:
            keywords[k] += 1

  # Print the keywords dictionary after processing all comments
  print(keywords) # Moved the print statement to the end of the main function

if __name__ == "__main__":
  main()

"""# Visualizing the data with “matplotlib”"""

! pip install matplotlib

"""# To use matplotlib library, we have to import it:"""

import matplotlib.pyplot as plt

"""# After print(keywords) instruction, add the following code:"""

import matplotlib.pyplot as plt
import requests
from bs4 import BeautifulSoup

# Define the main function
def main():
    url = "https://news.ycombinator.com/item?id=42919502"
    response = requests.get(url)

    soup = BeautifulSoup(response.content, "html.parser")
    # Find all elements with class="ind" and indent level = 0
    elements = soup.find_all(class_="ind", indent=0)
    # For each of these elements, find the next element with class

"""# Execute the code.
# Generate requirements.txt:
"""

!pip freeze > requirements.txt